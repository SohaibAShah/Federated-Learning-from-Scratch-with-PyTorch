{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf4e9986",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Advanced Federated Learning with Scaffold and ResNet on CIFAR-100\n",
    "\n",
    "Welcome to the next level of federated learning! This tutorial will guide you through implementing Scaffold (Stochastic Controlled Averaging for Federated Learning), a powerful and robust algorithm designed to handle non-IID data. We'll use a more advanced ResNet model and the challenging CIFAR-100 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53861293",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Data Preparation (Non-IID CIFAR-100)\n",
    "\n",
    "First, let's prepare our environment and the more complex CIFAR-100 dataset. We will again simulate non-IID data conditions to show why Scaffold is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce7006aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision import datasets, transforms\n",
    "import copy\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78dbbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a device for training\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data transformations for CIFAR-100\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43617df",
   "metadata": {},
   "source": [
    "### Download and load the CIFAR100 training and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9cd9bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.CIFAR100('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR100('./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b91d1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been partitioned among 10 clients, each with a disjoint set of classes.\n"
     ]
    }
   ],
   "source": [
    "NUM_CLIENTS = 10\n",
    "CLIENT_BATCH_SIZE = 32\n",
    "\n",
    "# Function to create non-IID data partitions for CIFAR-100\n",
    "def create_non_iid_partitions_cifar100(dataset, num_clients, classes_per_client=10):\n",
    "    class_indices = [[] for _ in range(100)]\n",
    "    for i, (_, label) in enumerate(dataset):\n",
    "        class_indices[label].append(i)\n",
    "    \n",
    "    client_indices = [[] for _ in range(num_clients)]\n",
    "    \n",
    "    # Assign a non-overlapping set of classes to each client\n",
    "    classes_per_client_list = np.array_split(np.arange(100), num_clients)\n",
    "    \n",
    "    for client_idx, client_classes in enumerate(classes_per_client_list):\n",
    "        for class_idx in client_classes:\n",
    "            client_indices[client_idx].extend(class_indices[class_idx])\n",
    "    \n",
    "    partitions = [data.Subset(dataset, random.sample(indices, len(indices))) for indices in client_indices]\n",
    "    return partitions\n",
    "\n",
    "client_data = create_non_iid_partitions_cifar100(train_dataset, NUM_CLIENTS)\n",
    "client_trainloaders = [data.DataLoader(d, batch_size=CLIENT_BATCH_SIZE, shuffle=True) for d in client_data]\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"Data has been partitioned among {NUM_CLIENTS} clients, each with a disjoint set of classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433e78f",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "We're using CIFAR-100, which has 100 classes, and the data normalization values are different.\n",
    "\n",
    "create_non_iid_partitions_cifar100: This function creates a stark non-IID distribution by giving each client a completely separate set of classes (e.g., client 1 gets classes 0-9, client 2 gets classes 10-19, and so on). This is a strong test for Scaffold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92d3ad",
   "metadata": {},
   "source": [
    "## Step 2: Defining the Advanced ResNet Model\n",
    "\n",
    "We'll use a simplified ResNet architecture, which is a powerful CNN that uses \"residual blocks\" to improve training in deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c440a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple ResNet block\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False) \n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12cc53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simplified ResNet model\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_blocks, num_classes=100):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.layer1 = self._make_layer(ResNetBlock, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(ResNetBlock, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(ResNetBlock, 64, num_blocks[2], stride=2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for s in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, s))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Instantiate our ResNet model\n",
    "# We'll use a ResNet-18 like architecture for this tutorial\n",
    "resnet_model = ResNet([2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3c4bc",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "BasicBlock: This class defines a core building block of a ResNet. It contains two convolutional layers and a \"shortcut\" connection that adds the input x to the output of the convolutional layers.\n",
    "\n",
    "ResNet: This class orchestrates multiple BasicBlock layers to create the full network. The _make_layer function helps build these layers systematically.\n",
    "\n",
    "This model is significantly deeper and more powerful than the simple CNN from the last tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d5c2f5",
   "metadata": {},
   "source": [
    "## Step 3: The Client-side Training with Scaffold\n",
    "\n",
    "Scaffold's key innovation is the use of \"control variates\" to correct for client drift. Each client maintains a local control variate c_k and the server maintains a global control variate c. The client's local gradient update is corrected using these variates.\n",
    "\n",
    "The client's training step is modified as follows:\n",
    "w_k\n",
    "leftarroww_kâˆ’\n",
    "eta(\n",
    "nablaF_k(w_k)âˆ’c_k+c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "931968c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to get model parameters as a flat vector\n",
    "def get_params_vector(model):\n",
    "    return torch.cat([p.data.view(-1) for p in model.parameters()])\n",
    "\n",
    "# A helper function to set model parameters from a flat vector\n",
    "def set_params_vector(model, param_vec):\n",
    "    start = 0\n",
    "    for p in model.parameters():\n",
    "        num_params = p.numel()\n",
    "        p.data.copy_(param_vec[start:start+num_params].view(p.size()))\n",
    "        start += num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8825c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client training function for Scaffold\n",
    "def client_training_scaffold(model, global_model_state_dict, c_global, c_local, trainloader, lr=0.01, epochs=1):\n",
    "    local_model = copy.deepcopy(model).to(DEVICE)\n",
    "    local_model.train()\n",
    "    \n",
    "    # Load global model state and initialize optimizer\n",
    "    local_model.load_state_dict(global_model_state_dict)\n",
    "    optimizer = optim.SGD(local_model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Get initial local model parameters\n",
    "    w_initial = get_params_vector(local_model)\n",
    "\n",
    "    # Initialize client control variate from the server's global variate\n",
    "    c_local_client = copy.deepcopy(c_local).to(DEVICE)\n",
    "    c_global_server = c_global.to(DEVICE)\n",
    "\n",
    "    for epoch in range(epochs):  # Single epoch for simplicity\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = local_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # --- Scaffold's Gradient Correction ---\n",
    "            # Get local gradients\n",
    "            local_grad = torch.cat([p.grad.data.view(-1) for p in local_model.parameters()])\n",
    "            \n",
    "            # Compute the corrected gradient: grad - c_local + c_global\n",
    "            corrected_grad = local_grad - c_local_client + c_global_server\n",
    "\n",
    "            # Update parameters with the corrected gradient\n",
    "            for p, g in zip(local_model.parameters(), corrected_grad.view_as(local_grad)):\n",
    "                p.data.add_(g, alpha=-lr)\n",
    "            \n",
    "    # Calculate the new client control variate\n",
    "    w_final = get_params_vector(local_model)\n",
    "    # c_new_local = c_local - c_global + (w_initial - w_final) / (len(trainloader) * lr)\n",
    "    # The formula is slightly more complex, but we'll use a simpler form for demonstration\n",
    "    c_new_local = c_local_client - c_global_server + (w_initial.to(DEVICE) - w_final.to(DEVICE)) / (len(trainloader.dataset) * lr)\n",
    "    \n",
    "    return local_model.state_dict(), c_new_local.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c1eb87",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "The client function now takes c_global and c_local as inputs.\n",
    "\n",
    "The most important part is the gradient correction line: corrected_grad = local_grad - c_local_client + c_global_server. This is where Scaffold's magic happens, preventing client drift.\n",
    "\n",
    "The client's local parameters are updated using this corrected gradient instead of the raw local gradient.\n",
    "\n",
    "The function returns both the updated model parameters and the new client control variate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ace9f",
   "metadata": {},
   "source": [
    "## Step 4: Server-side Aggregation\n",
    "\n",
    "The server's role in Scaffold is to aggregate the model weights (just like FedAvg) and to update the global control variate based on the client's updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b5e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_parameters(client_updates):\n",
    "    \"\"\"Aggregates parameters from multiple clients using FedAvg.\"\"\"\n",
    "    if not client_updates:\n",
    "        return None\n",
    "\n",
    "    global_state_dict = copy.deepcopy(client_updates[0])\n",
    "    \n",
    "    # Zero-out the global state dictionary\n",
    "    for name in global_state_dict.keys():\n",
    "        global_state_dict[name] = torch.zeros_like(global_state_dict[name])\n",
    "\n",
    "    for client_state_dict in client_updates:\n",
    "        for name, param in client_state_dict.items():\n",
    "            # Correction: Only aggregate floating-point parameters\n",
    "            if param.dtype == torch.float32:\n",
    "                global_state_dict[name] += param / len(client_updates)\n",
    "            else:\n",
    "                # For non-float params (like num_batches_tracked), just copy from the first client\n",
    "                # as they are not aggregated in the same way.\n",
    "                global_state_dict[name] = client_state_dict[name]\n",
    "    \n",
    "    return global_state_dict\n",
    "\n",
    "def update_global_control_variate(c_global, c_local_updates):\n",
    "    \"\"\"Updates the global control variate in Scaffold.\"\"\"\n",
    "    if not c_local_updates:\n",
    "        return c_global\n",
    "\n",
    "    new_c_global = copy.deepcopy(c_global)\n",
    "    \n",
    "    for c_update in c_local_updates:\n",
    "        # Correction: Move the client update to the same device as the global variate\n",
    "        new_c_global += c_update.to(new_c_global.device) / len(c_local_updates)\n",
    "        \n",
    "    return new_c_global"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a490e676",
   "metadata": {},
   "source": [
    "## Step 5: The Federated Training Loop\n",
    "\n",
    "Finally, we tie everything together. The main loop will now manage not only the global model but also the global control variate and each client's local control variate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "920ad139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial global model accuracy: 0.67%\n"
     ]
    }
   ],
   "source": [
    "def server_evaluation(model, dataloader):\n",
    "    \"\"\"Evaluates the global model on the server's test set.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Initialize the global model and control variates\n",
    "global_model = ResNet([2, 2, 2]).to(DEVICE)\n",
    "global_c_variate = torch.zeros_like(get_params_vector(global_model))\n",
    "client_c_variates = [torch.zeros_like(get_params_vector(global_model)) for _ in range(NUM_CLIENTS)]\n",
    "\n",
    "print(f\"Initial global model accuracy: {server_evaluation(global_model, test_dataloader):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4a72f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Federated Round 1/10 ---\n",
      "Server selects clients: [6, 1, 5, 3, 2]\n",
      "Client 6 is training...\n",
      "Client 1 is training...\n",
      "Client 5 is training...\n",
      "Client 3 is training...\n"
     ]
    }
   ],
   "source": [
    "# Federated Learning Main Loop\n",
    "NUM_ROUNDS = 10\n",
    "CLIENTS_PER_ROUND = 5\n",
    "CLIENT_LEARNING_RATE = 0.01\n",
    "\n",
    "for round_num in range(NUM_ROUNDS):\n",
    "    print(f\"\\n--- Starting Federated Round {round_num + 1}/{NUM_ROUNDS} ---\")\n",
    "    \n",
    "    participating_clients_indices = random.sample(range(NUM_CLIENTS), CLIENTS_PER_ROUND)\n",
    "    print(f\"Server selects clients: {participating_clients_indices}\")\n",
    "\n",
    "    global_model_state_dict = global_model.state_dict()\n",
    "    client_updates = []\n",
    "    client_c_variate_updates = []\n",
    "    \n",
    "    for client_idx in participating_clients_indices:\n",
    "        # Simulate local training on each client\n",
    "        print(f\"Client {client_idx} is training...\")\n",
    "        client_dataloader = client_trainloaders[client_idx]\n",
    "        \n",
    "        # Get the client's current local control variate\n",
    "        c_local = client_c_variates[client_idx]\n",
    "\n",
    "        # Run client training with Scaffold\n",
    "        local_state_dict, new_c_local = client_training_scaffold(\n",
    "            global_model, global_model_state_dict, global_c_variate, c_local, client_dataloader, lr=CLIENT_LEARNING_RATE, epochs=3\n",
    "        )\n",
    "        \n",
    "        # Store the updates\n",
    "        client_updates.append(local_state_dict)\n",
    "        client_c_variate_updates.append(new_c_local)\n",
    "\n",
    "        # Update the client's local control variate for the next round\n",
    "        client_c_variates[client_idx] = new_c_local\n",
    "\n",
    "    # Server aggregates model updates and control variate updates\n",
    "    new_global_state_dict = aggregate_parameters(client_updates)\n",
    "    print(f\"Global model parameters Aggregated.\")\n",
    "\n",
    "    if new_global_state_dict:\n",
    "        global_model.load_state_dict(new_global_state_dict)\n",
    "        print(f\"Global model parameters updated.\")\n",
    "\n",
    "\n",
    "    global_c_variate = update_global_control_variate(global_c_variate, client_c_variate_updates)\n",
    "\n",
    "    accuracy = server_evaluation(global_model, test_dataloader)\n",
    "    print(f\"Global model accuracy after round {round_num + 1}: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Federated Learning Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5061b06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UPFall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

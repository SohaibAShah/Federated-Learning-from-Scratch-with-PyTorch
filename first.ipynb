{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef9f6347",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Federated Learning from Scratch with PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568fe08",
   "metadata": {},
   "source": [
    "### Step 1: Setup and Data Preparation\n",
    "\n",
    "First, let's install PyTorch and prepare our data. Federated learning is all about distributed data, so we'll simulate this by splitting a single dataset (MNIST) into several smaller datasets, one for each \"client.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "932a3fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import copy # We will use this to deep-copy our model to clients\n",
    "import random # For client selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e894887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a device for training (GPU if available, otherwise CPU)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16378630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data transformations for our dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4cc99",
   "metadata": {},
   "source": [
    "#### Download and load the MNIST training and test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c2920a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a real-world scenario, this data would already be on the clients' devices.\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e4fb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the number of clients we'll simulate\n",
    "NUM_CLIENTS = 10\n",
    "CLIENT_BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc4c6a",
   "metadata": {},
   "source": [
    "#### Partition the data for each client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a1e7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll split the training data equally among the 10 clients.\n",
    "client_data = torch.utils.data.random_split(train_dataset, \n",
    "                                            [len(train_dataset) // NUM_CLIENTS] * NUM_CLIENTS\n",
    "                                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "029b1b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for each client's data\n",
    "client_trainloaders = [\n",
    "    DataLoader(data, batch_size=CLIENT_BATCH_SIZE, shuffle=True) for data in client_data\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "304cd838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a single test DataLoader for the server's evaluation\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f940f01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been partitioned among 10 clients.\n",
      "Each client has 6000 samples.\n",
      "Test dataset has 10000 samples.\n",
      "Test DataLoader created with batch size 128.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data has been partitioned among {NUM_CLIENTS} clients.\")\n",
    "print(f\"Each client has {len(client_data[0])} samples.\")\n",
    "print(f\"Test dataset has {len(test_dataset)} samples.\")\n",
    "print(f\"Test DataLoader created with batch size {test_dataloader.batch_size}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c148ee",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "torch, nn, optim: Standard PyTorch imports for building and training neural networks.\n",
    "\n",
    "copy: We'll use copy.deepcopy to create independent copies of the global model for each client.\n",
    "\n",
    "random: To randomly select a subset of clients for each training round.\n",
    "\n",
    "datasets.MNIST: We use the MNIST dataset because it's simple and a great starting point.\n",
    "\n",
    "torch.utils.data.random_split: This function is our \"magic wand\" for simulating decentralized data. It splits the train_dataset into 10 non-overlapping subsets, each representing a single client's private data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b23b62",
   "metadata": {},
   "source": [
    "## Step 2: Defining the Neural Network Model\n",
    "\n",
    "We'll use a simple Multi-Layer Perceptron (MLP) for this task. The model is defined once and will be used by both the server (as the global model) and the clients (as their local models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "858427d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model architecture\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        # 28x28 images, so input size is 784\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10) # Output layer for 10 classes (digits 0-9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32a5d52",
   "metadata": {},
   "source": [
    "### Code Explanation:\n",
    "\n",
    "This is a standard PyTorch nn.Module class. It defines the structure of our model.\n",
    "\n",
    "The forward method specifies how data flows through the network.\n",
    "\n",
    "We're using a simple architecture: flatten the image, pass it through two fully connected layers with a ReLU activation, and a final layer for the 10 output classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e02c0f5",
   "metadata": {},
   "source": [
    "## Step 3: The Client-side Training Loop\n",
    "\n",
    "Each client needs a function to perform local training. This function will take the client's data and the current global model, train it for a few epochs, and return the updated model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f878e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (879769786.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 22\u001b[0;36m\u001b[0m\n\u001b[0;31m    for images, labels in trainloafer:\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "def client_training(model, trainloader, epochs=1):\n",
    "    \"\"\"\n",
    "    Performs a single round of local training on a client's data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The global model parameters from the server.\n",
    "        trainloader (DataLoader): The client's local data loader.\n",
    "        epochs (int): Number of local epochs to train for.\n",
    "\n",
    "    Returns:\n",
    "        OrderedDict: The updated state_dict (model parameters) after local training.\n",
    "    \"\"\"\n",
    "    # Create a local copy of the model\n",
    "    local_model = copy.deepcopy(model).to(DEVICE)\n",
    "    local_model.train()  # Set the model to training mode\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = local_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    return local_model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f498749",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UPFall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
